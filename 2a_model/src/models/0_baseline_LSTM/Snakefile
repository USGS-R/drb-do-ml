import os
import tensorflow as tf
import numpy as np
import pandas as pd

from river_dl.preproc_utils import asRunConfig
from river_dl.preproc_utils import prep_all_data
from river_dl.evaluate import combined_metrics
from river_dl.postproc_utils import plot_obs, plot_ts
from river_dl.predict import predict_from_arbitrary_data
from river_dl.train import train_model
from river_dl import loss_functions as lf
from model import LSTMModel

out_dir = os.path.join(config['out_dir'], config['exp_name'])
loss_function = lf.multitask_rmse(config['lambdas'])

rule all:
    input:
        expand("{outdir}/{metric_type}_metrics.csv",
                outdir=out_dir,
                metric_type=['overall', 'reach'],
        )
        

rule as_run_config:
    output:
        "{outdir}/asRunConfig.yml"
    run:
        asRunConfig(config,output[0])


rule prep_io_data:
    input:
        "../../../out/well_observed_train_inputs.zarr",
        "../../../out/well_observed_train_do.zarr",
    output:
        "{outdir}/prepped.npz"
    run:
        prep_all_data(
                  x_data_file=input[0],
                  y_data_file=input[1],
                  x_vars=config['x_vars'],
                  y_vars_finetune=config['y_vars'],
                  spatial_idx_name='site_id',
                  time_idx_name='date',
                  train_start_date=config['train_start_date'],
                  train_end_date=config['train_end_date'],
                  val_start_date=config['val_start_date'],
                  val_end_date=config['val_end_date'],
                  test_start_date=config['test_start_date'],
                  test_end_date=config['test_end_date'],
                  out_file=output[0],
                  trn_offset = config['trn_offset'],
                  tst_val_offset = config['tst_val_offset'])


model = LSTMModel(
    config['hidden_size'],
    recurrent_dropout=config['recurrent_dropout'],
    dropout=config['dropout'],
    num_tasks=len(config['y_vars'])
)


# Finetune/train the model on observations
rule train:
    input:
        "{outdir}/prepped.npz",
    output:
        directory("{outdir}/train_weights/"),
        #directory("{outdir}/best_val_weights/"),
        "{outdir}/train_log.csv",
        "{outdir}/train_time.txt",
    group: 'train_predict_evaluate'
    run:
        optimizer = tf.optimizers.Adam(learning_rate=config['finetune_learning_rate']) 
        model.compile(optimizer=optimizer, loss=loss_function)
        data = np.load(input[0], allow_pickle=True)
        nsegs = len(np.unique(data["ids_trn"]))
        train_model(model,
                    x_trn = data['x_trn'],
                    y_trn = data['y_obs_trn'],
                    epochs = config['pt_epochs'],
                    batch_size = nsegs,
                    x_val = data['x_val'],
                    y_val = data['y_obs_val'],
                    # I need to add a trailing slash here. Otherwise the wgts
                    # get saved in the "outdir"
                    weight_dir = output[0] + "/",
                    #best_val_weight_dir = output[1] + "/",
                    log_file = output[1],
                    time_file = output[2],
                    early_stop_patience=config['early_stopping'])


rule make_predictions:
    input:
        "{outdir}/train_weights/",
        "../../../out/well_observed_train_val_inputs.zarr",
        "{outdir}/prepped.npz",
    output:
        "{outdir}/preds.feather",
    group:
        'train_predict_evaluate'
    run:
        weight_dir = input[0] + "/"
        model.load_weights(weight_dir)
        preds = predict_from_arbitrary_data(raw_data_file=input[1],
                                            pred_start_date="1980-01-01",
                                            pred_end_date="2019-01-01",
                                            train_io_data=input[2],
                                            model=model, 
                                            spatial_idx_name='site_id',
                                            time_idx_name='date')
        preds.reset_index(drop=True).to_feather(output[0])


rule make_validation_predictions:
    input:
        "{outdir}/preds.feather"
    output:
        "{outdir}/val_preds.feather"
    group:
        "train_predict_evaluate"
    run:
        df_preds = pd.read_feather(input[0])
        all_sites = df_preds.site_id.unique()
        trn_sites = all_sites[(~np.isin(all_sites, config["validation_sites"])) &
                              (~np.isin(all_sites, config["test_sites"]))]

        df_val_sites = df_preds[(df_preds.site_id.isin(config['validation_sites'])) & 
                                (df_preds.date < config['val_end_date'][0])]

        df_val_times = df_preds[(df_preds.site_id.isin(trn_sites)) & 
                                (df_preds.date < config['val_end_date'][0]) &
                                (df_preds.date >= config['val_start_date'][0])]

        df_val_preds = pd.concat([df_val_sites, df_val_times], axis=0)
        df_val_preds.reset_index(drop=True).to_feather(output[0])


rule make_train_predictions:
    input:
        "{outdir}/preds.feather",
    output:
        "{outdir}/trn_preds.feather",
    group:
        'train_predict_evaluate'
    run:
        df_preds = pd.read_feather(input[0])
        all_sites = df_preds.site_id.unique()
        trn_sites = all_sites[(~np.isin(all_sites, config["validation_sites"])) &
                              (~np.isin(all_sites, config["test_sites"]))]
        df_preds_trn_sites = df_preds[df_preds.site_id.isin(trn_sites)]
        df_trn = df_preds_trn_sites[(df_preds_trn_sites.date >= config['train_start_date'][0]) &
                                    (df_preds_trn_sites.date < config['train_end_date'][0])]
        df_trn.reset_index(drop=True).to_feather(output[0])

 
 
def get_grp_arg(wildcards):
     if wildcards.metric_type == 'overall':
         return None
     elif wildcards.metric_type == 'month':
         return 'month'
     elif wildcards.metric_type == 'reach':
         return 'seg_id_nat'
     elif wildcards.metric_type == 'month_reach':
         return ['seg_id_nat', 'month']
 
 
rule combine_metrics:
     input:
          "../../../out/well_observed_train_do.zarr",
          "{outdir}/trn_preds.feather",
          "{outdir}/val_preds.feather"
     output:
          "{outdir}/{metric_type}_metrics.csv"
     group: 'train_predict_evaluate'
     params:
         grp_arg = get_grp_arg
     run:
         combined_metrics(obs_file=input[0],
                          pred_trn=input[1],
                          pred_val=input[2],
                          spatial_idx_name='site_id',
                          time_idx_name='date',
                          group=params.grp_arg,
                          outfile=output[0])
 
 
rule plot_prepped_data:
     input:
         "{outdir}/prepped.npz",
     output:
         "{outdir}/{variable}_part_{partition}.png",
     run:
         plot_obs(input[0],
                  wildcards.variable,
                  output[0],
                  spatial_idx_name="site_id",
                  time_idx_name="date",
                  partition=wildcards.partition)


